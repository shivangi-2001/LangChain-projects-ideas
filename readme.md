# ðŸ’¬ Conversational ChatBot with LangChain & OpenAI

## ðŸ“Œ Goal


Build a **stateful chatbot** that keeps track of the conversation using memory. Each input from the user is passed to the LLM along with the entire message history for context-aware replies.

## Components Used


- âœ… **ChatOpenAI**
  - Connects to OpenAIâ€™s chat models like gpt-3.5-turbo or gpt-4.

- âœ… **ConversationBufferMemory**
  - Stores all previous messages and returns them in the format compatible with chat models. Helps maintain context between turns.

- âœ… **Prompt Components:**
   - **ChatPromptTemplate** â€“ Builds a structured prompt including the current message and past history.
   - **HumanMessagePromptTemplate** â€“ Template for the user message ({content}).
   - **MessagesPlaceholder** â€“ Inserts past messages from memory into the prompt.

- âœ… **LLMChain**
  - Connects the ChatOpenAI model with the structured prompt and memory. Executes a single inference call.

## ðŸ§  Pipeline Flow

--- 
```python
Input: User enters text
    â†“
1ï¸âƒ£ HumanMessagePromptTemplate: Formats the input
    â†“
2ï¸âƒ£ ChatPromptTemplate: Combines input + past memory
    â†“
3ï¸âƒ£ LLMChain: Sends the prompt to OpenAI's chat model
    â†“
4ï¸âƒ£ ConversationBufferMemory: Appends response to history
    â†“
Output: Model's reply based on full conversation context

```

## ðŸ” Loop Logic

---

The **chatbot** runs in a loop:

```python
while True:
    content = input('Enter content: ')
    result = chain({"content": content})
    print(result["text"])
```
Each new message is passed to the **LLMChain**, which uses both the message and previous conversation stored in memory to generate a response.

## ðŸ§ª Example


``Enter content: Who won the World Cup in 2018?``
``â†’ France won the 2018 FIFA World Cup. ``

``Enter content: Who was the captain?``
``â†’ Hugo Lloris was the captain of the French team in 2018.``
---

# LangChain Sequential Chain

## ðŸ“Œ Goal


Automate a multi-step text generation pipeline using LangChain:

1. **First chain** - Generate code 
2. **Second chain** - Generate test for that code

## Components used

âœ… **PromptTemplate**

Used to create custom prompts with variables.

Example:
```python
PromptTemplate(
    input_variables=['language', 'task'],
    template="Write a very short {language} function that will {task}"
)
```

âœ… **LLMChain**

A single-step chain: LLM + PromptTemplate â†’ Output
Used to execute a single prompt using an LLM like OpenAI or ChatGPT.

âœ… **SequentialChain**

Multi-step chain: Combines multiple LLMChain instances.
Automatically passes output of one chain to the next.
Used to build complex workflows.

## ðŸ§  Pipeline Flow


```python
Input: task + language
    â†“
1ï¸âƒ£ code_chain: Generates code
    â†“
2ï¸âƒ£ test_chain: Verifies code by writing a test
    â†“
Output: code + test
```

## ðŸ§ª Example


```commandline
python script.py --task "sort an array" --language "Python"
```

Output:
```commandline
{
  "code": "def sort_array(arr): return sorted(arr)",
  "test": "def test_sort_array(): assert sort_array([3,1,2]) == [1,2,3]"
}
```

---

# Terminal Chatbot

This is a simple terminal-based chatbot powered by OpenAI's chat models and LangChain. 

To simulate a real conversation with a chatbot like ChatGPT, we use the following types of messages:

1. **System Message** â€“ Sets the behavior or personality of the chatbot. Typically defined by developers.
2. **User Message** â€“ Sent by the user to the chatbot (i.e., your query).
3. **Assistant Message** â€“ The response generated by the chatbot model.

Note: ChatGPT does not retain memory of previous interactions between calls. So, to continue a conversation, you must send the entire message history each time.

LangChain provides abstractions over OpenAIâ€™s message types:

| **ChatGPT Type** | 	**LangChain Equivalent** |
|------------------|---------------------------|
| System Message   | 	SystemMessage            | 
| User Message	    | HumanMessage              | 
| Assistant Message | 	AIMessage                | 

To create meaningful chat interactions, we use a ChatPromptTemplate, which allows dynamic injection of variables into the prompt.

In this project, we use two variables in the prompt:

**subject** â€“ Injected into the system message to define the chatbot's behavior.

**query** â€“ Injected into the human message to represent the userâ€™s input.

## ðŸ§° Components Used

âœ… **ChatPromptTemplate:**
 A template for formatting system/human/AI messages.
```python
prompt = ChatPromptTemplate(
    input_variables=["content"],
    messages=[
        HumanMessagePromptTemplate.from_template("{content}")
    ]
)
```
âœ…**HumanMessagePromptTemplate:**

Represents a human (user) message in the conversation flow.

## Memory Usage

In LangChain, Memory is used to store and manage information during a chain's execution. It is not limited to just storing chat messages â€” it can handle any type of state required for the chain's operation.

### Role of Memory in a Chain
When you run a chain, the memory component plays two major roles:

**Before Model Execution:**
Memory receives the input variables and has the ability to inject additional context or variables into the prompt.

**After Model Execution:**
Once the language model returns an output, memory receives this output. It can then inspect and store useful information for future interactions (e.g., conversation history, summaries, tokens).

### ðŸ§ª Quick Tip: Enable verbose=True in the chain
``chain = LLMChain(..., verbose=True)``
> This will print exactly how the memory content is injected into each prompt and what data is being saved afterward. Useful for debugging!

### Data Flow (Simplified) 

| Step                  | 	Component | 	Action                                    |
| ----- |------------|--------------------------------------------|
| 1 |	Input Variables	| Received from the user                     |
| 2 |	Memory	Injects | additional context if needed               |
| 3	| ChatPromptTemplate | Formats the full prompt                    | 
| 4	| Language Model | 	Produces a response                       |
| 5	| Memory	| Receives and stores output (if configured) | 
| 6	| Output	| Returned to the user                       |

### Types of Memory in LangChain
LangChain offers several built-in memory classes for different use cases:

**ConversationBufferMemory**

Stores the entire conversation history in memory (as plain text). Best for maintaining full dialogue context.

**ConversationBufferWindowMemory**

Maintains only the last N messages of the conversation â€” useful for limiting token usage.

**ConversationTokenBufferMemory**
Tracks conversation based on token count instead of message count. Ideal for managing input limits to models like GPT-3.5/4.

**CombinedMemory**

Allows combining multiple memory types into one â€” for example, keeping both a token buffer and a summary.


## Example Use Case
If you're building a chatbot that should "remember" the conversation context:

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```
**Prompt with memory injection**
```python
prompt = ChatPromptTemplate(
    input_variables=["content"],
    messages=[
        MessagePlaceholder(varaible_name="message"),
        HumanMessagePromptTemplate.from_template("{content}")
    ]
)
```
Then pass this memory into your chain:
```python
chain = LLMChain(
    llm=chat,
    prompt=prompt,
    memory=memory,
    output_key='result'
)
```

This setup allows the chatbot to maintain a dynamic memory of the conversation and use it in responses.


## ðŸ’¾ Persistent Memory with File Storage
If you want to persist memory between program runs, you can use FileChatMessageHistory to store conversations in a file:

```python
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory

memory = ConversationBufferMemory(
   memory_key="chat_history",
   chat_memory=FileChatMessageHistory(file_path="Terminal ChatBot/message.json"),
   return_messages=True
)
```
You can also use SQL, PostgreSQL, or other databases for message storage.

However, storing all full-length messages can get overwhelming and memory-heavy for long conversations.

## ðŸ“ Memory Optimization: Conversation Summary

To avoid overload, use ==ConversationSummaryMemory==, which summarizes the conversation and stores only the key points.

```python
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(
    llm=chat,
    memory_key="summary"
)
```

This memory type integrates with the prompt template and stores concise summaries instead of entire message histories.

---

# ðŸ§  Question Answering Model with LangChain

Weâ€™re building a Question-Answering (Q/A) system that can search across various types of document files 
(PDF, TXT, CSV, XLSX, etc.) and retrieve relevant content to answer user queries. We'll use LangChain to 
manage document loading, processing, and semantic retrieval using embeddings.

## Loading Your Documents
LangChain provides loader classes to ingest content from different file types such as .txt, .pdf, .csv, .md, .json, .xlsx, etc.

```
from langchain.document_loaders import TextLoader

loader = TextLoader("facts.txt")
docs = loader.load()
```
The docs is a list of Document objects:
```commandline
[Document(page_content="...", metadata={"source": "facts.txt"})]
```

## ðŸ” Basic Retrieval (Without Embeddings)
Initially, we can split documents into small chunks and **manually match facts** using keyword overlap with the userâ€™s query:

Steps:
1. Load the document.
2. Split the content into fact-based chunks.
3. Compare the query against each chunk.
4. Return the most relevant facts.

âš ï¸ This approach fails when the user query is vague or uses different wording.

### Text chunks
```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=200,
    chunk_overlap=0,
)

# loading the files (replace with file you want to divide)
loader = TextLoader("facts.txt")
docs = loader.load_and_split(
    text_splitter=text_splitter
)

print(docs)

```


## ðŸ§  Semantic Search with Embeddings

To improve accuracy, we use embeddings, which help the system understand the meaning behind queries and textâ€”even when the words differ.

### What is an Embedding?
An **embedding** is a vector (usually 700â€“1500 dimensions) that represents a piece of text in numerical space based on its meaning.

- Values range between -1 and 1.
- Captures semantic features like topic, tone, etc.

### Similarity Metrics:
| Method               | 	Description   | 
|----------------------|----------------|
| **L2 Distance**      | 	Measures how close two vectors are in Euclidean space|
| **Cosine Similarity** | 	Measures the angle between two vectors (better for high-dim spaces)|

## ðŸ› ï¸ Embedding-Based Search Pipeline

1. **Split Text into Chunks**
   - Break large documents into smaller, manageable segments.
2. **Generate Embeddings for Each Chunk**
   - Use a model like OpenAI, HuggingFace, or SentenceTransformers.
3. **Store in a Vector Database**
   - Examples: FAISS, Chroma, Weaviate, Pinecone, Qdrant
4. **On User Query:**
   - Embed the query.
   - Search for semantically similar document chunks using cosine similarity.
   - Retrieve top matching contexts to answer the question.

```python
from langchain_community.embeddings import OpenAIEmbeddings

embedding = OpenAIEmbeddings()
emb = embeddings.embed_query("hi there")

print(emb)
```

## ðŸ”¢ Why We Use Embedding Models

There are a variety of models available to generate embeddingsâ€”each with different characteristics:

1. SentenceTransformer (all-mpnet-base-v2)
    - Dimension: 768
    - Open-source model from ***HuggingFace***
    - Works well for semantic similarity tasks in local/offline setups.
2. OpenAI Embeddings (text-embedding-ada-002)
   - Dimension: 1536
   - Cloud-based, highly optimized for semantic search.
   - Requires an ***OpenAI API***  key and internet access.

These models convert text into numerical vectors that capture **semantic meaning**, allowing us to compare and retrieve similar content effectively.

---

## ðŸ§± Why Use ChromaDB?

To store and search these embedding vectors efficiently, we use ChromaDB, a fast and lightweight vector database.

Install via:
``pip install chromadb``

### ðŸ” Why Chroma?
- âœ… **Simple and Local-first** â€“ Runs out-of-the-box on your machine.
- âš¡ **Fast Vector Search** â€“ Optimized for cosine similarity and dense retrieval.
- ðŸ”Œ **LangChain Integration** â€“ Seamlessly works with LangChainâ€™s retrievers.
- ðŸ’¾ **No External Services Needed** â€“ Perfect for local and private use-cases.

> **ChromaDB** helps us persist embeddings and perform fast similarity search when answering user queries.

```python
db = Chroma.from_documents(
    documents=docs, #docs from the loader
    embedding=embeddings, #embedding model
    persist_directory="QA/emb" #directiory that store vectors
)

results = db.similarity_search(
    "what is an interesting fact about english language",
    k=2
)
```

The ``similarity_search()`` function is used to find the most relevant document chunks based on 
the semantic meaning of a userâ€™s query. In the line ``results = db.similarity_search("what is an interesting fact about english language", k=2),``
the query is first converted into an embedding vector and then compared against the stored 
document vectors in the vector database (such as ChromaDB). The function returns the 
top ``k`` resultsâ€”in this case, the 2 most similar chunksâ€”based on cosine similarity. 
This allows the system to retrieve meaningful context even when the user's question is phrased differently from the original text, 
making it a core part of embedding-based retrieval in a question-answering pipeline.

This is the core of the retrieval step in a ``Retrieval-Augmented Generation (RAG)`` pipeline: 
- Without understanding exact keywords, the model can find conceptually related text. 
- Enables your system to pull relevant context before passing it to a language model like ChatGPT.

In this process, every time we run the program, it regenerates and stores the same text embeddings, leading to duplication and inefficiency. 
To avoid this, we separate the logic into two parts: one file handles text loading, splitting, embedding generation, and storing into ChromaDB, 
while another file focuses solely on querying the vector database and performing question-answering. 

## Retriever
A retriever is an object designed to accept a user query (string) and return a list of relevant document chunks. 
For any object to be used as a retriever in LangChain, it must implement the method ``get_relevant_documents()``, which takes in a string
and returns a list of ``Document`` objects that match the meaning of the query.

In our workflow, we first run the QA.py file to load, split, and embed documents, storing the vectors inside the QA/emb directory using ChromaDB. Then, we run the vector_embedding.py file, where we load those vectors and use a retriever to fetch matching content. We pass this retriever to a RetrievalQA chain with the "stuff" method, which combines both system and user prompts to guide the LLM in answering the question accurately.
```python
chat = ChatOpenAI()
embeddings=OpenAIEmbeddings()
db = Chroma(
    embedding_function=embeddings,
    persist_directory="emb"
)

retriever = db.as_retriever()

chain = RetrievalQA.from_chain_type(
    llm = chat,
    retriever = retriever,
    chain_type = "stuff"
)

result = chain.run("what is an interesting fact about Neil Armstrong ?")
```
