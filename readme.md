# LangChain Sequential Chain

## üìå Goal

----

Automate a multi-step text generation pipeline using LangChain:

1. **First chain** - Generate code 
2. **Second chain** - Generate test for that code

## Components used

----
‚úÖ **PromptTemplate**

Used to create custom prompts with variables.

Example:
```python
PromptTemplate(
    input_variables=['language', 'task'],
    template="Write a very short {language} function that will {task}"
)
```

‚úÖ **LLMChain**

A single-step chain: LLM + PromptTemplate ‚Üí Output
Used to execute a single prompt using an LLM like OpenAI or ChatGPT.

‚úÖ **SequentialChain**

Multi-step chain: Combines multiple LLMChain instances.
Automatically passes output of one chain to the next.
Used to build complex workflows.

## üß† Pipeline Flow

----

```python
Input: task + language
    ‚Üì
1Ô∏è‚É£ code_chain: Generates code
    ‚Üì
2Ô∏è‚É£ test_chain: Verifies code by writing a test
    ‚Üì
Output: code + test
```

## üß™ Example

----

```commandline
python script.py --task "sort an array" --language "Python"
```

Output:
```commandline
{
  "code": "def sort_array(arr): return sorted(arr)",
  "test": "def test_sort_array(): assert sort_array([3,1,2]) == [1,2,3]"
}
```


# Terminal Chatbot

This is a simple terminal-based chatbot powered by OpenAI's chat models and LangChain. 

To simulate a real conversation with a chatbot like ChatGPT, we use the following types of messages:

1. **System Message** ‚Äì Sets the behavior or personality of the chatbot. Typically defined by developers.
2. **User Message** ‚Äì Sent by the user to the chatbot (i.e., your query).
3. **Assistant Message** ‚Äì The response generated by the chatbot model.

Note: ChatGPT does not retain memory of previous interactions between calls. So, to continue a conversation, you must send the entire message history each time.

LangChain provides abstractions over OpenAI‚Äôs message types:

| **ChatGPT Type** | 	**LangChain Equivalent** |
|------------------|---------------------------|
| System Message   | 	SystemMessage            | 
| User Message	    | HumanMessage              | 
| Assistant Message | 	AIMessage                | 

To create meaningful chat interactions, we use a ChatPromptTemplate, which allows dynamic injection of variables into the prompt.

In this project, we use two variables in the prompt:

**subject** ‚Äì Injected into the system message to define the chatbot's behavior.

**query** ‚Äì Injected into the human message to represent the user‚Äôs input.

## üß∞ Components Used

---

‚úÖ **ChatPromptTemplate:**
 A template for formatting system/human/AI messages.
```python
prompt = ChatPromptTemplate(
    input_variables=["content"],
    messages=[
        HumanMessagePromptTemplate.from_template("{content}")
    ]
)
```
‚úÖ**HumanMessagePromptTemplate:**

Represents a human (user) message in the conversation flow.

---
## Memory Usage

In LangChain, Memory is used to store and manage information during a chain's execution. It is not limited to just storing chat messages ‚Äî it can handle any type of state required for the chain's operation.

### Role of Memory in a Chain
When you run a chain, the memory component plays two major roles:

**Before Model Execution:**
Memory receives the input variables and has the ability to inject additional context or variables into the prompt.

**After Model Execution:**
Once the language model returns an output, memory receives this output. It can then inspect and store useful information for future interactions (e.g., conversation history, summaries, tokens).

### üß™ Quick Tip: Enable verbose=True in the chain
``chain = LLMChain(..., verbose=True)``
> This will print exactly how the memory content is injected into each prompt and what data is being saved afterward. Useful for debugging!

### Data Flow (Simplified) 

| Step                  | 	Component | 	Action                                    |
| ----- |------------|--------------------------------------------|
| 1 |	Input Variables	| Received from the user                     |
| 2 |	Memory	Injects | additional context if needed               |
| 3	| ChatPromptTemplate | Formats the full prompt                    | 
| 4	| Language Model | 	Produces a response                       |
| 5	| Memory	| Receives and stores output (if configured) | 
| 6	| Output	| Returned to the user                       |

### Types of Memory in LangChain
LangChain offers several built-in memory classes for different use cases:

**ConversationBufferMemory**

Stores the entire conversation history in memory (as plain text). Best for maintaining full dialogue context.

**ConversationBufferWindowMemory**

Maintains only the last N messages of the conversation ‚Äî useful for limiting token usage.

**ConversationTokenBufferMemory**
Tracks conversation based on token count instead of message count. Ideal for managing input limits to models like GPT-3.5/4.

**CombinedMemory**

Allows combining multiple memory types into one ‚Äî for example, keeping both a token buffer and a summary.

___

## Example Use Case
If you're building a chatbot that should "remember" the conversation context:

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```
**Prompt with memory injection**
```python
prompt = ChatPromptTemplate(
    input_variables=["content"],
    messages=[
        MessagePlaceholder(varaible_name="message"),
        HumanMessagePromptTemplate.from_template("{content}")
    ]
)
```
Then pass this memory into your chain:
```python
chain = LLMChain(
    llm=chat,
    prompt=prompt,
    memory=memory,
    output_key='result'
)
```

This setup allows the chatbot to maintain a dynamic memory of the conversation and use it in responses.

---

## üíæ Persistent Memory with File Storage
If you want to persist memory between program runs, you can use FileChatMessageHistory to store conversations in a file:

```python
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    chat_memory=FileChatMessageHistory(file_path="message.json"),
    return_messages=True
)
```
You can also use SQL, PostgreSQL, or other databases for message storage.

However, storing all full-length messages can get overwhelming and memory-heavy for long conversations.

## üìù Memory Optimization: Conversation Summary

To avoid overload, use ==ConversationSummaryMemory==, which summarizes the conversation and stores only the key points.

```python
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(
    llm=chat,
    memory_key="summary"
)
```

This memory type integrates with the prompt template and stores concise summaries instead of entire message histories.

