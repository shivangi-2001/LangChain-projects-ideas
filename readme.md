# üí¨ Conversational ChatBot with LangChain & OpenAI

## üìå Goal

---

Build a **stateful chatbot** that keeps track of the conversation using memory. Each input from the user is passed to the LLM along with the entire message history for context-aware replies.

## Components Used

--- 

- ‚úÖ **ChatOpenAI**
  - Connects to OpenAI‚Äôs chat models like gpt-3.5-turbo or gpt-4.

- ‚úÖ **ConversationBufferMemory**
  - Stores all previous messages and returns them in the format compatible with chat models. Helps maintain context between turns.

- ‚úÖ **Prompt Components:**
   - **ChatPromptTemplate** ‚Äì Builds a structured prompt including the current message and past history.
   - **HumanMessagePromptTemplate** ‚Äì Template for the user message ({content}).
   - **MessagesPlaceholder** ‚Äì Inserts past messages from memory into the prompt.

- ‚úÖ **LLMChain**
  - Connects the ChatOpenAI model with the structured prompt and memory. Executes a single inference call.

## üß† Pipeline Flow

--- 
```python
Input: User enters text
    ‚Üì
1Ô∏è‚É£ HumanMessagePromptTemplate: Formats the input
    ‚Üì
2Ô∏è‚É£ ChatPromptTemplate: Combines input + past memory
    ‚Üì
3Ô∏è‚É£ LLMChain: Sends the prompt to OpenAI's chat model
    ‚Üì
4Ô∏è‚É£ ConversationBufferMemory: Appends response to history
    ‚Üì
Output: Model's reply based on full conversation context

```

## üîÅ Loop Logic

---

The **chatbot** runs in a loop:

```python
while True:
    content = input('Enter content: ')
    result = chain({"content": content})
    print(result["text"])
```
Each new message is passed to the **LLMChain**, which uses both the message and previous conversation stored in memory to generate a response.

## üß™ Example

---

``Enter content: Who won the World Cup in 2018?``
``‚Üí France won the 2018 FIFA World Cup. ``

``Enter content: Who was the captain?``
``‚Üí Hugo Lloris was the captain of the French team in 2018.``
---

---

# LangChain Sequential Chain

## üìå Goal

----

Automate a multi-step text generation pipeline using LangChain:

1. **First chain** - Generate code 
2. **Second chain** - Generate test for that code

## Components used

----
‚úÖ **PromptTemplate**

Used to create custom prompts with variables.

Example:
```python
PromptTemplate(
    input_variables=['language', 'task'],
    template="Write a very short {language} function that will {task}"
)
```

‚úÖ **LLMChain**

A single-step chain: LLM + PromptTemplate ‚Üí Output
Used to execute a single prompt using an LLM like OpenAI or ChatGPT.

‚úÖ **SequentialChain**

Multi-step chain: Combines multiple LLMChain instances.
Automatically passes output of one chain to the next.
Used to build complex workflows.

## üß† Pipeline Flow

----

```python
Input: task + language
    ‚Üì
1Ô∏è‚É£ code_chain: Generates code
    ‚Üì
2Ô∏è‚É£ test_chain: Verifies code by writing a test
    ‚Üì
Output: code + test
```

## üß™ Example

----

```commandline
python script.py --task "sort an array" --language "Python"
```

Output:
```commandline
{
  "code": "def sort_array(arr): return sorted(arr)",
  "test": "def test_sort_array(): assert sort_array([3,1,2]) == [1,2,3]"
}
```

---

---

# Terminal Chatbot

This is a simple terminal-based chatbot powered by OpenAI's chat models and LangChain. 

To simulate a real conversation with a chatbot like ChatGPT, we use the following types of messages:

1. **System Message** ‚Äì Sets the behavior or personality of the chatbot. Typically defined by developers.
2. **User Message** ‚Äì Sent by the user to the chatbot (i.e., your query).
3. **Assistant Message** ‚Äì The response generated by the chatbot model.

Note: ChatGPT does not retain memory of previous interactions between calls. So, to continue a conversation, you must send the entire message history each time.

LangChain provides abstractions over OpenAI‚Äôs message types:

| **ChatGPT Type** | 	**LangChain Equivalent** |
|------------------|---------------------------|
| System Message   | 	SystemMessage            | 
| User Message	    | HumanMessage              | 
| Assistant Message | 	AIMessage                | 

To create meaningful chat interactions, we use a ChatPromptTemplate, which allows dynamic injection of variables into the prompt.

In this project, we use two variables in the prompt:

**subject** ‚Äì Injected into the system message to define the chatbot's behavior.

**query** ‚Äì Injected into the human message to represent the user‚Äôs input.

## üß∞ Components Used

---

‚úÖ **ChatPromptTemplate:**
 A template for formatting system/human/AI messages.
```python
prompt = ChatPromptTemplate(
    input_variables=["content"],
    messages=[
        HumanMessagePromptTemplate.from_template("{content}")
    ]
)
```
‚úÖ**HumanMessagePromptTemplate:**

Represents a human (user) message in the conversation flow.

---
## Memory Usage

In LangChain, Memory is used to store and manage information during a chain's execution. It is not limited to just storing chat messages ‚Äî it can handle any type of state required for the chain's operation.

### Role of Memory in a Chain
When you run a chain, the memory component plays two major roles:

**Before Model Execution:**
Memory receives the input variables and has the ability to inject additional context or variables into the prompt.

**After Model Execution:**
Once the language model returns an output, memory receives this output. It can then inspect and store useful information for future interactions (e.g., conversation history, summaries, tokens).

### üß™ Quick Tip: Enable verbose=True in the chain
``chain = LLMChain(..., verbose=True)``
> This will print exactly how the memory content is injected into each prompt and what data is being saved afterward. Useful for debugging!

### Data Flow (Simplified) 

| Step                  | 	Component | 	Action                                    |
| ----- |------------|--------------------------------------------|
| 1 |	Input Variables	| Received from the user                     |
| 2 |	Memory	Injects | additional context if needed               |
| 3	| ChatPromptTemplate | Formats the full prompt                    | 
| 4	| Language Model | 	Produces a response                       |
| 5	| Memory	| Receives and stores output (if configured) | 
| 6	| Output	| Returned to the user                       |

### Types of Memory in LangChain
LangChain offers several built-in memory classes for different use cases:

**ConversationBufferMemory**

Stores the entire conversation history in memory (as plain text). Best for maintaining full dialogue context.

**ConversationBufferWindowMemory**

Maintains only the last N messages of the conversation ‚Äî useful for limiting token usage.

**ConversationTokenBufferMemory**
Tracks conversation based on token count instead of message count. Ideal for managing input limits to models like GPT-3.5/4.

**CombinedMemory**

Allows combining multiple memory types into one ‚Äî for example, keeping both a token buffer and a summary.

___

## Example Use Case
If you're building a chatbot that should "remember" the conversation context:

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```
**Prompt with memory injection**
```python
prompt = ChatPromptTemplate(
    input_variables=["content"],
    messages=[
        MessagePlaceholder(varaible_name="message"),
        HumanMessagePromptTemplate.from_template("{content}")
    ]
)
```
Then pass this memory into your chain:
```python
chain = LLMChain(
    llm=chat,
    prompt=prompt,
    memory=memory,
    output_key='result'
)
```

This setup allows the chatbot to maintain a dynamic memory of the conversation and use it in responses.

---

## üíæ Persistent Memory with File Storage
If you want to persist memory between program runs, you can use FileChatMessageHistory to store conversations in a file:

```python
from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory

memory = ConversationBufferMemory(
   memory_key="chat_history",
   chat_memory=FileChatMessageHistory(file_path="Terminal ChatBot/message.json"),
   return_messages=True
)
```
You can also use SQL, PostgreSQL, or other databases for message storage.

However, storing all full-length messages can get overwhelming and memory-heavy for long conversations.

## üìù Memory Optimization: Conversation Summary

To avoid overload, use ==ConversationSummaryMemory==, which summarizes the conversation and stores only the key points.

```python
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(
    llm=chat,
    memory_key="summary"
)
```

This memory type integrates with the prompt template and stores concise summaries instead of entire message histories.

---

---


# üß† Question Answering Model with LangChain

We‚Äôre building a Question-Answering (Q/A) system that can search across various types of document files 
(PDF, TXT, CSV, XLSX, etc.) and retrieve relevant content to answer user queries. We'll use LangChain to 
manage document loading, processing, and semantic retrieval using embeddings.

## Loading Your Documents
LangChain provides loader classes to ingest content from different file types such as .txt, .pdf, .csv, .md, .json, .xlsx, etc.

```
from langchain.document_loaders import TextLoader

loader = TextLoader("facts.txt")
docs = loader.load()
```
The docs is a list of Document objects:
```commandline
[Document(page_content="...", metadata={"source": "facts.txt"})]
```

## üîç Basic Retrieval (Without Embeddings)
Initially, we can split documents into small chunks and **manually match facts** using keyword overlap with the user‚Äôs query:

Steps:
1. Load the document.
2. Split the content into fact-based chunks.
3. Compare the query against each chunk.
4. Return the most relevant facts.

‚ö†Ô∏è This approach fails when the user query is vague or uses different wording.

### Text chunks
```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=200,
    chunk_overlap=0,
)

# loading the files (replace with file you want to divide)
loader = TextLoader("facts.txt")
docs = loader.load_and_split(
    text_splitter=text_splitter
)

print(docs)

```


## üß† Semantic Search with Embeddings

To improve accuracy, we use embeddings, which help the system understand the meaning behind queries and text‚Äîeven when the words differ.

### What is an Embedding?
An **embedding** is a vector (usually 700‚Äì1500 dimensions) that represents a piece of text in numerical space based on its meaning.

- Values range between -1 and 1.
- Captures semantic features like topic, tone, etc.

### Similarity Metrics:
| Method               | 	Description   | 
|----------------------|----------------|
| **L2 Distance**      | 	Measures how close two vectors are in Euclidean space|
| **Cosine Similarity** | 	Measures the angle between two vectors (better for high-dim spaces)|

## üõ†Ô∏è Embedding-Based Search Pipeline

1. **Split Text into Chunks**
   - Break large documents into smaller, manageable segments.
2. **Generate Embeddings for Each Chunk**
   - Use a model like OpenAI, HuggingFace, or SentenceTransformers.
3. **Store in a Vector Database**
   - Examples: FAISS, Chroma, Weaviate, Pinecone, Qdrant
4. **On User Query:**
   - Embed the query.
   - Search for semantically similar document chunks using cosine similarity.
   - Retrieve top matching contexts to answer the question.

```python
from langchain_community.embeddings import OpenAIEmbeddings

embedding = OpenAIEmbeddings()
emb = embeddings.embed_query("hi there")

print(emb)
```

## üî¢ Why We Use Embedding Models

There are a variety of models available to generate embeddings‚Äîeach with different characteristics:

1. SentenceTransformer (all-mpnet-base-v2)
    - Dimension: 768
    - Open-source model from ***HuggingFace***
    - Works well for semantic similarity tasks in local/offline setups.
2. OpenAI Embeddings (text-embedding-ada-002)
   - Dimension: 1536
   - Cloud-based, highly optimized for semantic search.
   - Requires an ***OpenAI API***  key and internet access.

These models convert text into numerical vectors that capture **semantic meaning**, allowing us to compare and retrieve similar content effectively.

---

## üß± Why Use ChromaDB?

To store and search these embedding vectors efficiently, we use ChromaDB, a fast and lightweight vector database.

Install via:
``pip install chromadb``

### üîç Why Chroma?
- ‚úÖ **Simple and Local-first** ‚Äì Runs out-of-the-box on your machine.
- ‚ö° **Fast Vector Search** ‚Äì Optimized for cosine similarity and dense retrieval.
- üîå **LangChain Integration** ‚Äì Seamlessly works with LangChain‚Äôs retrievers.
- üíæ **No External Services Needed** ‚Äì Perfect for local and private use-cases.

> **ChromaDB** helps us persist embeddings and perform fast similarity search when answering user queries.

```python
db = Chroma.from_documents(
    documents=docs, #docs from the loader
    embedding=embeddings, #embedding model
    persist_directory="emb" #directiory that store vectors
)

results = db.similarity_search(
    "what is an interesting fact about english language",
    k=2
)
```

The ``similarity_search()`` function is used to find the most relevant document chunks based on 
the semantic meaning of a user‚Äôs query. In the line ``results = db.similarity_search("what is an interesting fact about english language", k=2),``
the query is first converted into an embedding vector and then compared against the stored 
document vectors in the vector database (such as ChromaDB). The function returns the 
top ``k`` results‚Äîin this case, the 2 most similar chunks‚Äîbased on cosine similarity. 
This allows the system to retrieve meaningful context even when the user's question is phrased differently from the original text, 
making it a core part of embedding-based retrieval in a question-answering pipeline.

This is the core of the retrieval step in a Retrieval-Augmented Generation (RAG) pipeline: 
- Without understanding exact keywords, the model can find conceptually related text. 
- Enables your system to pull relevant context before passing it to a language model like ChatGPT.

In these process when every we run the program we are duplicating the vector embedding and storing then so to prevent that to happen we separate the ``text_splitting`` & ``db chromaDB`` into one file.
and other file belong to actual question and answering.
